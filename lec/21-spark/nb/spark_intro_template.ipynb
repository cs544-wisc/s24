{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfc07bc-8862-4a68-9ff0-0b31dd3f569d",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe0117e-6b91-4651-8f21-c509f5e63ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575d6b2-2d6f-48ed-8d2a-7c6c385902a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5ba47-e1ff-4612-abfa-cf949d5dd03c",
   "metadata": {},
   "source": [
    "Chain of operations enable us to set parameters inside the `Builder` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3147e-29e9-4085-a4c6-b6b9f6e2bdce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ff7e7-1619-4aa0-95df-249970cf9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.builder.appName(\"cs544\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd6523-021e-423e-85ef-9dd036478312",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.builder.appName(\"cs544\").master(\"spark://boss:7077\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc0b43-047b-4197-906c-7b709700697a",
   "metadata": {},
   "source": [
    "#### Web server access\n",
    "\n",
    "Once you create the spark session, you will be able to access the web server that listens on 4040.\n",
    "\n",
    "Connect to `localhost:4040` on your browser and that should give you detailed information about your spark cluster. \"Executors\" tab will give you details about the cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38b408-d62c-416d-bbb7-1d0336db699f",
   "metadata": {},
   "source": [
    "`sparkContext` is the entry point for all RDD related things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363cf9d-c564-4d6e-aacf-76104349438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097d34a-5eaa-4428-bfb8-d1850e133f58",
   "metadata": {},
   "source": [
    "Let's create a list containing numbers from 0 to 1M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3727d6-9e28-4630-972e-2484278508b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = list(range(1_000_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24363b9-8084-4628-9017-cd631a6c0cd6",
   "metadata": {},
   "source": [
    "### RDD creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e92791-eb6a-4eaa-a346-b12b9888109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979e3b1-4a5f-4e99-804d-abe41fd76b86",
   "metadata": {},
   "source": [
    "### `lambda` syntax\n",
    "\n",
    "- anonymous functions\n",
    "- `lambda ARGUMENTS: EXPRESSION`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d18371-93e9-497b-bc37-29652498df78",
   "metadata": {},
   "source": [
    "### Transformation: `map`\n",
    "\n",
    "Let's compute inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a844a60-ea1d-4bf9-8f9b-dcf4f19a6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverses = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ede217-27d3-498a-8a24-01a3ce8a5d4f",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "- Action is what triggers the actual computation (or work)\n",
    "  \n",
    "We could get all results using `collect`, but be careful that is a lot of data to store in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70d8e1-abe4-47d9-ab93-7a35827d7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverses.collect() # ACTION to get all the numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6dc6d6-f52b-4348-a2b3-ffeceec06111",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "Let's get top N results instead using `take(<N>)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f291a-f3e4-469f-a9a6-8c2cb9395149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any potential problems in running this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea630d-7709-492b-893d-77a7e3717e5f",
   "metadata": {},
   "source": [
    "How can we fix the `ZeroDivisionError` error?\n",
    "\n",
    "### Filter\n",
    "\n",
    "Let's filter out any values <= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f74ce2-64ef-4c52-8c6b-5ced014becef",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverses = rdd.???.map(lambda x: 1/x)\n",
    "inverses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19831e8-876b-4440-ba79-622f841ca887",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "Let's compute mean of all the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd233ae3-21e0-4d5b-a7b2-2c12b07aa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762bd584-fa40-4c62-a709-a5443697b37e",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "Number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d020e2c-330c-4616-ad54-973cca30227f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "263d96d5-f1ab-4aed-bdb2-25ede21d7a09",
   "metadata": {},
   "source": [
    "Let's create 10 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a5bc8-9831-43e2-84b6-b384a7c80ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(nums, ???)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c31c3-900a-446c-9b8e-8164a31edc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverses = rdd.filter(lambda x: x > 0).map(lambda x: 1/x)\n",
    "inverses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5d5c5-be95-452a-b9d3-7ec4d1c5e90f",
   "metadata": {},
   "source": [
    "#### How to read spark job progress bar?\n",
    "\n",
    "For example, `4 + 2 / 10` means:\n",
    "- 4 tasks are done\n",
    "- 2 tasks are running\n",
    "- 10 tasks in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38ef53-d9a7-4074-96d3-6ac2198e22a8",
   "metadata": {},
   "source": [
    "### RDD caching\n",
    "\n",
    "RDD sampling: `<rdd>.sample(...)`\n",
    "- Psuedorandomness (seed) is not always possible in spark sampling because partitions might be different everytime. However, if you have same partitions every time, then seed will be deterministic.\n",
    "- So, how can we achieve psuedorandomness?\n",
    "  - Sample\n",
    "  - Save results in a file\n",
    "  - Only use that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9486e-0ae3-45ca-9fcb-cfc0a43f6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5233d45-5d84-47bf-8363-d93f8bb092a6",
   "metadata": {},
   "source": [
    "How long does it take to compute mean on the sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21601b3c-11b0-48a5-ac58-d3e015d688e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ab261e9-851a-4c96-ad3a-70d8b7fd535b",
   "metadata": {},
   "source": [
    "Let's cache the results. This is fast because no work is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50874786-b37d-4c7c-a8fc-231596ae5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b4426-f7af-4446-bffa-46d90060da28",
   "metadata": {},
   "source": [
    "The first time you \"use\" cached rdd, it will be slower than just running the computation itself. Why? Well it is doing the task work + extra caching work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264649f1-f2c5-49ba-849b-63403bdf2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(sample.mean())\n",
    "end_time = time.time()\n",
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290eb99-34df-44f3-812c-618e569f3e3b",
   "metadata": {},
   "source": [
    "Let's try it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c032497-f80a-4206-a54b-ee0588fe09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(sample.mean())\n",
    "end_time = time.time()\n",
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e3b70-4434-47ca-8c57-754d7a100d7e",
   "metadata": {},
   "source": [
    "Doesn't give us much improvement, why not? \n",
    "\n",
    "We started with a big dataset (1M numbers). Sampling leads to narrow partitions because it doesn't want to shuffle data across partitions.\n",
    "\n",
    "Solution: re-partition after sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc7978-0da6-4312-b630-9983c43adff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = rdd.sample(True, fraction=0.1, seed=544).???.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5bdd1c-2d0b-45fc-8b10-809aac264c86",
   "metadata": {},
   "source": [
    "Again will be slower first time, as we are doing the compute and caching work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f232f2-12a9-4eef-ba8b-2b147b59192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(sample.mean())\n",
    "end_time = time.time()\n",
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8603459d-fa1a-4639-a903-345c26ff154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(sample.mean())\n",
    "end_time = time.time()\n",
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761416d6-6156-4cfa-8f3c-f65aabbd888e",
   "metadata": {},
   "source": [
    "Better performance than before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01140642-3c64-48e8-815e-70cf4e4c4d66",
   "metadata": {},
   "source": [
    "### Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cab4e-c629-48af-9471-b6d14afaaf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://ms.sites.cs.wisc.edu/cs544/data/ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4eb15c-b10b-479c-897b-3ea18a5c3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f5650-5581-4854-a4e6-845c94346093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023861f-4504-43f1-810c-3987915140ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df), type(df.rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e69177-45d0-47ee-a675-59c3c8c3e695",
   "metadata": {},
   "source": [
    "Let's take a peek at first 10 lines within this spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce203448-6443-4830-9e47-411bf0fec329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c6b4c72-f952-43ac-a806-35e646630718",
   "metadata": {},
   "source": [
    "Why doesn't this work? Where is our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae63b68-ac0f-43af-9d31-eb521534c292",
   "metadata": {},
   "source": [
    "#### Moving data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5b67e-cb1f-45fb-8d83-0df2a2ae804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350b105-b477-46bb-b276-772484f54e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5870c349-0133-4c44-8d7a-166139742072",
   "metadata": {},
   "source": [
    "Let's read the data from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e14326-a6f9-4c17-9f37-4bf99aeb788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b4de1-ab79-4767-9b5e-b8c8488368f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a9c90-54fe-4080-a605-39cd817d9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13f9e7-1e33-48ae-bb90-8b5db2ca0e6f",
   "metadata": {},
   "source": [
    "Let's convert spark dataframe to pandas dataframe. **Be careful!** entire data might not fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29cd629-7341-4f94-8191-6d68e5cbdef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to first 10 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30386b-55eb-4c12-8a73-903fa2c94276",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.limit(10).toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b54f55-0be9-4e81-891d-27439cc1b6b3",
   "metadata": {},
   "source": [
    "#### Extract station ID using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b52e96-b4a3-4ffa-b0ad-5b59bacd126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df[\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6247981-58e2-4657-98c2-16c5704ae5ba",
   "metadata": {},
   "source": [
    "We can add station ID as a new column into the same pandas dataframe because it is mutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3f131-b1b9-4ac4-bcd2-a6cd09993599",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df[\"station\"] = pandas_df[\"value\"].str[:11]\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865cf52-1f20-48ce-9830-d347bd4a9c50",
   "metadata": {},
   "source": [
    "#### Extract station ID using Spark\n",
    "\n",
    "`from pyspark.sql.functions import col, expr`<br>\n",
    "`expr(<SQL>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28318987-8f5e-49f6-87cd-f19df6c1ce9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a24390-3818-4450-8a52-399524ddc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#substring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b097b-ddac-42a4-8da4-77710836f200",
   "metadata": {},
   "source": [
    "We **cannot** add station ID as a new column into the same spark dataframe because it is immutable. Recall that spark dataframe build on spark SQL which depends on RDD format, which is immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f20bbc-47c6-4db5-91e2-619a8770cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aac4c1-568f-4966-8c87-84bfed9c285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1131013-c518-4002-a94d-60c6ff0e4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.limit(10).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
