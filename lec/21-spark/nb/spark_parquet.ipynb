{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef683632-1a3a-4049-95e3-0741dac9a56e",
   "metadata": {},
   "source": [
    "## WARNING: do not run this notebook without swap enabled and make sure to stop the kernel in \"spark_intro\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6faef97-da45-4a61-bcd2-62ba53a64146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d6a86-5d93-43b2-8f96-18d46b88e716",
   "metadata": {},
   "source": [
    "### Spark + parquet\n",
    "\n",
    "Add swap space (caching for anonymous data):\n",
    "1. `sudo fallocate -l 1G /swapfile`\n",
    "2. `sudo chmod g-r /swapfile`\n",
    "3. `sudo chmod o-r /swapfile`\n",
    "4. `sudo mkswap /swapfile`\n",
    "5. `sudo swapon /swapfile`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa448a87-6df1-4ae7-953b-faef4ec35679",
   "metadata": {},
   "source": [
    "### SF fire dataset\n",
    "\n",
    "Data source: https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad12fa-5a7b-4782-99a1-f38a82618aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://ms.sites.cs.wisc.edu/cs544/data/sf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acedb5b-1ba4-4a9f-8724-a221d5520754",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip sf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fb3aa-2761-4937-b080-9348a14a98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa091d2-00ef-4fbb-9572-a2a72455ec28",
   "metadata": {},
   "source": [
    "Hive let's you take files in HDFS and converts them into tables in a database. Then, we can run SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a3844-53da-492c-84e3-2a91bd071ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35915ff5-0626-4ddf-8ec6-4d00617fa31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "471859200 / 1024**2 # min needed in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9932d1-e9e6-4d5c-8b9f-e77ccdda5835",
   "metadata": {},
   "source": [
    "Let's copy sf.csv into HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9e19d-967b-4517-9da1-68d0dd4f383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp sf.csv hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fcbba-9556-4b4f-9258-02bda2cd738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"hdfs://nn:9000/sf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9d3fb-ff8a-4930-a764-38cec3e22a68",
   "metadata": {},
   "source": [
    "Let's convert first three lines to pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829f30e-bc27-4748-8a7c-331a63011a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ae943-a3c4-4b60-ab51-d84399c020ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0565a49-661a-451f-aaac-683a187e04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db7448-afd3-4023-b753-b6a6a68ab4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c2e61-1da7-4a11-b465-54c04afd02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a7b88-df30-4d05-bb67-6fd1d82df44e",
   "metadata": {},
   "source": [
    "### How to transform the data with functions on columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f1b03-79aa-4e5e-b967-28dcafc1e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "col(\"Call Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4bc27-5754-410b-bcfa-5fbbb9f0449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr(\"Call Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4637c-3c5b-460f-b1f1-0eebe6ad719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col(\"Call Date\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef399f-3325-42ac-9a1a-4b21c90cf9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"`Call Date`\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a9822-f1a3-4af7-9b4b-da5b8ca0b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"`Call Date`\").alias(\"Date\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0715f773-fbfc-4440-9b7c-780d2325af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"to_date(`Call Date`, 'MM/dd/yyyy')\").alias(\"Date\")).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac89c85-514d-416b-8b88-0d261f55bf9c",
   "metadata": {},
   "source": [
    "#### GOAL: create a parquet file with this data, with no spaces in the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc14d3-6407-4db1-95d6-22f8a0bbf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col(c).alias(c.replace(\" \", \"_\")) for c in df.columns]\n",
    "columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436556ba-ec8a-4fd6-90fd-0ee4f453b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3afb9b-27b1-4fcb-9ad3-a122682bc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.select(columns)\n",
    " .write\n",
    " .format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .save(\"hdfs://nn:9000/sf.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ae1a3-cabf-4d13-ad40-57147ad44629",
   "metadata": {},
   "source": [
    "Let's check the files on HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e06230-a164-43f2-a4bb-da5b0b1af235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b2285-17c2-4ffd-b658-eead32ea9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs://nn:9000/sf.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88046ca-28e7-4fb8-9ac2-4a60d2a12b59",
   "metadata": {},
   "source": [
    "Let's read the data from the parquet file that we wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d2475-ed85-453d-a970-de5f1b11574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/sf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b8ed0c-06cc-46d6-81cd-df8b591c3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f0063-98e2-49c9-bee5-ab09965e1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8b320-4611-490b-baaa-96dff57cf8b1",
   "metadata": {},
   "source": [
    "Why does spark use fewer partitions now? Compression feature of parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269212d-b14f-450c-9729-24ba856d2657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
