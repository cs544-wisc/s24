{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9f9791-df33-4cdb-b08e-d6a07bf3c048",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "### Code to be executed before lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760c4c1-c34f-4ba2-a9f4-b6857e97a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dca847-54af-4284-97d8-0682e88a6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2e1dd-99ee-49e3-b84c-667800e78a2b",
   "metadata": {},
   "source": [
    "#### If you did not bring the Spark cluster down, you don't have to execute the below code. If you did bring it down, then please execute the below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669cde3-f636-494c-bdbf-7f5c19b5e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://ms.sites.cs.wisc.edu/cs544/data/sf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205896b-940b-4556-b327-1019616f31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip sf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067deb95-eda9-4dbc-b563-fb338cbe98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294e4e0-ab19-496c-980f-31df757e7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp sf.csv hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004b74e-dfd7-4d31-ad01-923dda7eca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd553c30-4870-4697-b96a-ae23507f3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col(c).alias(c.replace(\" \", \"_\")) for c in df.columns]\n",
    "df.select(cols).write.format(\"parquet\").mode(\"overwrite\").save(\"hdfs://nn:9000/sf.parquet\")\n",
    "df = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/sf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17bf1e-5bef-4c34-b600-bbe56219291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b8d8e-4993-4a5f-94b6-ff9709563cd6",
   "metadata": {},
   "source": [
    "### Lecture starts here\n",
    "\n",
    "### HIVE View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd547fb7-92d6-4af6-8d5d-5c896222dbe5",
   "metadata": {},
   "source": [
    "Let's rename \"Neighborhooods_-_Analysis_Boundaries\" to \"area\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642c9f16-458a-4d16-a80e-dcd377bbec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumnRenamed(\"Neighborhooods_-_Analysis_Boundaries\", \"area\").createOrReplaceTempView(\"calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994410b6-faa8-4c06-9d9d-a5c19004326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68df1a0-7316-4889-b633-8a68971bc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM calls LIMIT 3\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9811c3b-f7a4-4c6a-ab32-bd13318c4609",
   "metadata": {},
   "source": [
    "### HIVE table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0585d2-2aaf-410c-97ce-2c8fe2ff67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM calls\n",
    "WHERE Call_Type LIKE 'Odor%'\n",
    "\"\"\").write.mode(\"overwrite\").saveAsTable(\"stinky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac948c-ed87-4700-81d2-a685fabd4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9cc916-c59e-43f8-92f8-bdcba980669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM stinky LIMIT 3\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e3936-760e-4e03-8f76-8351e63e1cf4",
   "metadata": {},
   "source": [
    "### HIVE data on HDFS\n",
    "\n",
    "Where is it located?\n",
    "\n",
    "```python\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12804526-556e-41df-adb2-13256f253462",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs://nn:9000/user/hive/warehouse/stinky/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f00f04-cfe3-49a4-8b8b-3f980ec1618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6d792-e21d-43df-b7aa-b95686fbb71f",
   "metadata": {},
   "source": [
    "### Number of partitions: writing vs reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280c3a6-a227-4b42-a69d-837f95434ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM calls\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0ad9c-1b3e-4663-aad7-85c5fd59a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM stinky\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28546440-9b98-450a-914e-c3b6de0bda16",
   "metadata": {},
   "source": [
    "### Create DataFrame from HIVE view or table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775846d-8996-4ae0-9f75-b8ecc29b0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"calls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef2aaa-996f-4337-9115-a345b7709bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"stinky\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d17d4-8a68-465a-91a9-58f82a78014c",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f6e74-b16b-4d49-8982-257e08e8764d",
   "metadata": {},
   "source": [
    "#### What are the unique area column values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f2b2c-0616-42f0-9794-3dcc7a3e72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DISTINCT area FROM calls\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f7822-c13e-478b-890c-72b00311b12e",
   "metadata": {},
   "source": [
    "#### How many calls are there per area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f0ab2-a8c2-4ea0-85ef-b12e50d4ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = spark.sql(\"\"\"\n",
    "SELECT area, COUNT(*) as count\n",
    "FROM calls\n",
    "GROUP BY area\n",
    "ORDER BY count DESC\n",
    "\"\"\").toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15756a5d-28e9-40bb-8600-00756d798986",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.set_index(\"area\").plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb74e6d-cdda-46a8-b5ed-0bd5f6d3e2f1",
   "metadata": {},
   "source": [
    "#### How many calls are there per groups/type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af81f7c-c69f-47b1-a4c6-411c65411e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type_Group, Call_Type, COUNT(*) as count\n",
    "FROM calls\n",
    "GROUP BY Call_Type_Group, Call_Type\n",
    "\"\"\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d715928-444f-466b-a8a4-fd5ac243c813",
   "metadata": {},
   "source": [
    "#### For each call group, what percentage of calls are represented by the biggest type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b9653-6fd4-4611-a898-a793797d0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type_Group, MAX(count) / SUM(count)\n",
    "FROM (\n",
    "    SELECT Call_Type_Group, Call_Type, COUNT(*) as count\n",
    "    FROM calls\n",
    "    GROUP BY Call_Type_Group, Call_Type\n",
    ")\n",
    "GROUP BY Call_Type_Group\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ccffd-6d31-402b-bea2-17decd8a0e4b",
   "metadata": {},
   "source": [
    "Let's use DataFrame API to solve the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4e7f8-a984-4f83-babe-f583a2826d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.table(\"calls\")\n",
    " .groupby(\"Call_Type_Group\", \"Call_Type\")\n",
    " .count()\n",
    " .groupby(\"Call_Type_Group\")\n",
    " .agg(expr(\"MAX(count) / SUM(count)\").alias(\"perc\"))\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b383400-8ad3-4fa1-9d01-4db2d7743149",
   "metadata": {},
   "source": [
    "### Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5bf909-2d79-462f-afb5-f149c0e5a2bc",
   "metadata": {},
   "source": [
    "#### What are three smallest call numbers for each area?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e18d94-8d06-4e22-9c75-f909b873b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT area, Call_Number, row_number() OVER (PARTITION BY area ORDER BY Call_Number ASC) AS rownum\n",
    "FROM calls\n",
    "\"\"\").where(\"rownum <= 3\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbde7eb-000e-4ecc-ba47-3fc2670028d2",
   "metadata": {},
   "source": [
    "### Holidays dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88eede-60b5-4f02-8e8e-d6bbc6281692",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp holidays2.csv hdfs://nn:9000/holidays2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86359383-1995-4aa9-9178-62f37ebae23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read\n",
    " .format(\"csv\")\n",
    " .option(\"inferSchema\", True)\n",
    " .option(\"header\", True)\n",
    " .load(\"hdfs://nn:9000/holidays2.csv\")\n",
    " .createOrReplaceTempView(\"holidays\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c9553-40a8-4488-b1f0-d1412eb1dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"holidays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82bee9-d372-4653-b1bd-55f2b37aba82",
   "metadata": {},
   "source": [
    "### Joining the SF fire data with holidays data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac21ae4-ca80-4575-bda3-c32d99e6f078",
   "metadata": {},
   "source": [
    "SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2a0bd-4ce9-4af6-a342-e1c483c67c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM calls\n",
    "LIMIT 5\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0489019-bdb1-4b1e-9359-faf1d66baede",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM holidays\n",
    "LIMIT 5\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b480131-be05-4bcc-88a0-a255e0d678d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM calls\n",
    "INNER JOIN holidays ON calls.Call_Date = holidays.date\n",
    "LIMIT 5\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b053ed9-395f-493e-b522-5c2f8d71e01f",
   "metadata": {},
   "source": [
    "DataFrame version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e89a2-9e9e-425c-8c1c-73de41a3f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrames\n",
    "calls = spark.table(\"calls\")\n",
    "holidays = spark.table(\"holidays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0254edb-de06-4ff1-b01a-5a8ed974f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this doesn't trigger compute in Spark unlike pandas\n",
    "calls[\"Call_Date\"] == holidays[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc741f0-6e41-4648-a065-72f3564c0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls.join(holidays, on=calls[\"Call_Date\"] == holidays[\"date\"], how=\"inner\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa7743-1e82-4156-a521-264bde748e43",
   "metadata": {},
   "source": [
    "#### How many calls on each kind of holiday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705f0ef-e0b8-486a-bd36-263883ffceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(calls\n",
    " .join(holidays, on=calls[\"Call_Date\"] == holidays[\"date\"], how=\"inner\")\n",
    " .groupby(\"holiday\")\n",
    " .count()\n",
    " .orderBy(\"count\")\n",
    " .toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc054cf-f092-4489-9551-9eedf066e72e",
   "metadata": {},
   "source": [
    "#### What percent of fire dept calls are on holidays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b49b0f-c097-492a-80fb-e8df33f80720",
   "metadata": {},
   "outputs": [],
   "source": [
    "(calls\n",
    " .join(holidays, on=calls[\"Call_Date\"] == holidays[\"date\"], how=\"left\")\n",
    " .agg(expr(\"COUNT(holiday) / COUNT(*)\"))\n",
    " .toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5084db80-659d-4980-9d11-8b7b8a87a0b1",
   "metadata": {},
   "source": [
    "### Web server REST API\n",
    "\n",
    "Documentation: https://spark.apache.org/docs/latest/monitoring.html#rest-api\n",
    "\n",
    "```\n",
    "http://localhost:4040/api/v1/applications\n",
    "http://localhost:4040/api/v1/applications/{app_id}/executors\n",
    "# look for \"totalTasks\"\n",
    "```\n",
    "\n",
    "#### Appplications information\n",
    "```\n",
    "http://localhost:4040/api/v1/applications\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54fa83-ece5-4a51-84d8-bd5ff27b597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"http://localhost:4040/api/v1/applications\")\n",
    "r.raise_for_status()\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874a0ba-7098-4898-876a-d17e920b92b6",
   "metadata": {},
   "source": [
    "Extracting app id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a095313-c9cb-488a-97e3-023b1fbaea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = r.json()[0][\"id\"]\n",
    "app_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fc9fe-d653-44bf-a790-4c352168dc85",
   "metadata": {},
   "source": [
    "#### Executors information\n",
    "\n",
    "For example, how much work each executor has done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810106db-8e35-4235-9914-843b8870a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(f\"http://localhost:4040/api/v1/applications/{app_id}/executors\")\n",
    "r.raise_for_status()\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb9b59-b0b0-4aef-b744-e167b3d572d0",
   "metadata": {},
   "source": [
    "#### How many total tasks have been run by each executor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3995f2-392d-4f9a-afa4-fc2b764be58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[exec[\"totalTasks\"] for exec in r.json()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395bd636-3350-4b0d-b52c-5e64b01a3653",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Let's sample the data, create a single partition and try some caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bd86d-48f2-4b9c-9e2e-f7fb2cb487ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses StorageLevel as \"MEMORY_ONLY\"\n",
    "df = spark.table(\"calls\").sample(True, 0.01).repartition(1).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20869c7a-c5e3-4a38-a4fe-8cd82902b625",
   "metadata": {},
   "source": [
    "Let's count the number of rows in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f9ba0-24f4-4ccd-839c-3bbc61b539ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345d24d-a81a-4fa9-84df-0cd58e8d7474",
   "metadata": {},
   "source": [
    "Let's take a look at total tasks executed by each executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da73af-70cc-4668-adf5-c91c089d34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(f\"http://localhost:4040/api/v1/applications/{app_id}/executors\")\n",
    "r.raise_for_status()\n",
    "[exec[\"totalTasks\"] for exec in r.json()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75858bf-5987-4e0f-bce8-aa807c67ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating count computation 30 times\n",
    "for i in range(30):\n",
    "    df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56794e-1179-496c-b5b6-d51638ee9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(f\"http://localhost:4040/api/v1/applications/{app_id}/executors\")\n",
    "r.raise_for_status()\n",
    "[exec[\"totalTasks\"] for exec in r.json()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a52b6-2bdd-4f9c-9e1b-ac91cd7cf73b",
   "metadata": {},
   "source": [
    "How can we have both executors do the work when we do caching? We need to use `StorageLevel` as `MEMORY_ONLY_2`. \n",
    "<br>**Try it by yourself using `persist` method instead of `cache` method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643ff7d-8d5d-4387-9166-6bd16fc43439",
   "metadata": {},
   "source": [
    "### Hash partitioning\n",
    "\n",
    "What is a hash function?\n",
    "- takes anything (e.g., just some bytes containing some data)\n",
    "- returns a number (deteriministic, but ideally not with an obvious pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114396f9-8f5a-4c08-be31-e309e9507bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(b\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758fb3f-1166-47a6-a542-037303b344bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hash(b\"a\"))\n",
    "print(hash(b\"b\"))\n",
    "print(hash(b\"c\"))\n",
    "print(hash(b\"d\"))\n",
    "print(hash(b\"e\"))\n",
    "print(hash(b\"f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b85546-9eff-4ae9-a91e-75c25ab6f463",
   "metadata": {},
   "source": [
    "We can use modulo operator (`%`) to determine what hash partition a particular value should go into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04481632-08a7-433b-b77c-3d714b69419b",
   "metadata": {},
   "source": [
    "For example, if we need 5 partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d5b58-9919-4053-8519-afc81262cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hash(b\"a\") % 5)\n",
    "print(hash(b\"b\") % 5)\n",
    "print(hash(b\"c\") % 5)\n",
    "print(hash(b\"d\") % 5)\n",
    "print(hash(b\"e\") % 5)\n",
    "print(hash(b\"f\") % 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984dc0ff-62e8-4afa-b9e8-7608aff00ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_string = \"aaaabbbefghihijkllmlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e10ea9-7e49-4556-b6c2-085292bdae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [[], [], [], [], []]\n",
    "for letter in random_string:\n",
    "    partition_idx = hash(letter) % len(partitions)\n",
    "    partitions[partition_idx].append(letter)\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9feed9b-f791-4672-870c-7b9f785d1e2b",
   "metadata": {},
   "source": [
    "#### Spark execution explanation\n",
    "\n",
    "`.explain()` or `.explain(\"formatted\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c41553-361e-45dd-a4ec-01464036689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*) as count\n",
    "FROM calls\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfda8e-68e7-45e4-8e7a-d8311f24b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*) as count\n",
    "FROM calls\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084422d-cb06-4511-b2d6-e9adf91cf214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
