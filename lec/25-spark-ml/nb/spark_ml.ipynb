{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b46e20-65e6-4a40-851c-8331889550a5",
   "metadata": {},
   "source": [
    "# Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69e4fc-da97-4bd4-b04a-e62a268fddce",
   "metadata": {},
   "source": [
    "### Code to be executed before lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e168c28-7bbd-49eb-b444-0e0e14231608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dca847-54af-4284-97d8-0682e88a6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963eb76-9a9e-40bc-b0f5-ff77485026b2",
   "metadata": {},
   "source": [
    "### WARNING: do not keep multiple copies of `sf.csv` on your VM as it will eat up disk space\n",
    "\n",
    "Please make sure to move sf.csv into `25-spark-ml/nb` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294e4e0-ab19-496c-980f-31df757e7837",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cp sf.csv hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54bacc-b52a-4c25-93d2-2ba0f61de9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", True)\n",
    "      .option(\"inferSchema\", True)\n",
    "      .load(\"hdfs://nn:9000/sf.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1298818-83f6-444b-b8a0-4be5b16fd6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col(c).alias(c.replace(\" \", \"_\")) for c in df.columns]\n",
    "df.select(cols).write.mode(\"ignore\").format(\"parquet\").save(\"hdfs://nn:9000/sf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1ded3-ed8a-4e39-94cb-dd3a3272af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm hdfs://nn:9000/sf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea48b5-e012-4ae2-a53a-e40350f94e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read\n",
    " .format(\"parquet\")\n",
    " .load(\"hdfs://nn:9000/sf.parquet\")\n",
    " .createOrReplaceTempView(\"calls\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f7c34-f0c5-4dab-bffb-31262db80029",
   "metadata": {},
   "source": [
    "### Lecture starts here\n",
    "\n",
    "### Spark execution explanation\n",
    "`.explain()` or `.explain(\"formatted\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a96e33-dab7-4a47-896f-ac8e76062421",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*)\n",
    "FROM calls\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b2798-ff99-464a-9c5d-26807b09f451",
   "metadata": {},
   "source": [
    "### Bucketed data\n",
    "\n",
    "`bucketBy(<numBuckets>, <col>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b210d-00fb-4c7f-8265-af22aaa6a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# would work without sampling, just using it to make it faster\n",
    "(spark.table(\"calls\")\n",
    " .sample(True, 0.01)\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .bucketBy(10, \"Call_Type\")\n",
    " .saveAsTable(\"call_by_type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255a27c-c06e-4905-873c-4438fdbf625c",
   "metadata": {},
   "source": [
    "Let's repeat the same SQL query now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c547a3c-9fe4-4b3f-89c6-2e4118223373",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Call_Type, COUNT(*)\n",
    "FROM call_by_type\n",
    "GROUP BY Call_Type\n",
    "\"\"\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3f3c1-585c-46bf-b65b-ea2c753cf97c",
   "metadata": {},
   "source": [
    "### JOIN Algorithms (for a single machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52615ae-a01a-4069-9ed2-4f141744cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind_id, color\n",
    "fruits = [\n",
    "    (\"B\", \"Yellow\"),\n",
    "    (\"A\", \"Green\"),\n",
    "    (\"C\", \"Orange\"),\n",
    "    (\"A\", \"Red\"),\n",
    "    (\"C\", \"Purple\"),\n",
    "    (\"B\", \"Green\")\n",
    "]\n",
    "\n",
    "# kind_id, name (assume no duplicate kind_id's)\n",
    "kinds = [\n",
    "    (\"A\", \"Apple\"),\n",
    "    (\"B\", \"Banana\"),\n",
    "    (\"C\", \"Carrot\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd0f616-e604-47cc-9d26-72713541586a",
   "metadata": {},
   "source": [
    "#### GOAL: print Yellow Banana, Green Apple, etc (any order)\n",
    "\n",
    "### Option 1: Hash join\n",
    "- Move smaller table to in-memory Python `dict`\n",
    "- Iterate over larger table and find matches using `dict` lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda8b07-34ef-4973-9744-70edd05be001",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_lookup = dict(kinds)\n",
    "kind_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0d473-45c4-4fa9-a602-361294d38525",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kind_id, color in fruits:\n",
    "    print(color, kind_lookup[kind_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df450ba-e516-4282-9b8b-b3f6f857cbd1",
   "metadata": {},
   "source": [
    "### Option 2: sort merge join\n",
    "\n",
    "- Sort both tables (can be done using disk too)\n",
    "- Iterate over smaller table\n",
    "  - Conditionally iterate over bigger table to find matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15710b6-5d4d-49dc-b1e7-afc0c44feeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.sort()\n",
    "kinds.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3457f1-1d8c-458b-bbc1-7ae8c96720ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ecc7c8-5ebc-4d7e-b0a4-a30e3b3b136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b3c45-99ac-4187-a56a-509e30bfdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_idx = 0\n",
    "for kind_id, food_name in kinds:\n",
    "    while fruit_idx < len(fruits):\n",
    "        if fruits[fruit_idx][0] > kind_id:\n",
    "            break\n",
    "        elif fruits[fruit_idx][0] == kind_id:\n",
    "            print(fruits[fruit_idx][1], food_name)\n",
    "        fruit_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c176496-3f43-4c1e-863e-afd3c6a9f71a",
   "metadata": {},
   "source": [
    "### Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e75c5-f78a-45d7-ac60-b7d33ae821c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154e0dc-cc7a-45fa-ad8e-32f85aae1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x1\": np.random.randint(0, 10, 100).astype(float), \n",
    "                   \"x2\": np.random.randint(0, 3, 100).astype(float)})\n",
    "df[\"y\"] = df[\"x1\"] + df[\"x2\"] + np.random.rand(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf570f0-7057-49de-8c29-8862d06a4035",
   "metadata": {},
   "source": [
    "Let's convert pandas dataframe to Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5fc3a-5866-42c9-aaea-50e937a3fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c0e3e-acd1-45bf-94fd-094cfd786b63",
   "metadata": {},
   "source": [
    "Recall that seed in Spark is not truly deterministic overall (because everytime we might have new partitions), just deterministic at the partition level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842274a-33ae-4d44-9b2d-a72b7661caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.75, 0.25], seed=42)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8e0c4-6699-42d7-84af-935f1ee5652d",
   "metadata": {},
   "source": [
    "Let's write data to Parquet format and read the data from the Parquet file.\n",
    "We need to now use `mode(\"ignore\")` to make sure that we work with the deterministic sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab1841-8026-43be-8e0d-fdb29b086d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/train.parquet\")\n",
    "test.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142a7fb-c4af-46d1-a884-d3a43d54543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/train.parquet\")\n",
    "test = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e3cb3-6eea-4366-838b-ddfe7269be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca319f-b636-4acb-835a-cde1e240ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, DecisionTreeRegressionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd68d0-b44f-4d90-85f0-f7841a128a36",
   "metadata": {},
   "source": [
    "- `DecisionTreeRegressor`: unfit model\n",
    "- `DecisionTreeRegressionModel`: fitted model\n",
    "    - In Spark, names ending in \"Model\" are the fitted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5966d-5f01-41db-9a6b-53eaa1c049c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALWAYS needs a vector column - even for a single feature!\n",
    "# dt = DecisionTreeRegressor(featuresCol=\"x1\", labelCol=\"y\")\n",
    "# dt.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc1157-ecf5-42ac-b569-448a25166b43",
   "metadata": {},
   "source": [
    "### VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c02f5-d529-48ea-8988-a82a71a128da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0889870-0c9a-4e86-98af-d48061ce16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "va.transform(train).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947e68f-492a-4e2d-960f-3bb9cfe66bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"y\")\n",
    "\n",
    "model = dt.fit(va.transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a4fba-6010-473f-ac35-e89acbeeb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dt), type(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
