{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b46e20-65e6-4a40-851c-8331889550a5",
   "metadata": {},
   "source": [
    "# Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69e4fc-da97-4bd4-b04a-e62a268fddce",
   "metadata": {},
   "source": [
    "### Code to be executed before lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e168c28-7bbd-49eb-b444-0e0e14231608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, DecisionTreeRegressionModel\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dca847-54af-4284-97d8-0682e88a6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154e0dc-cc7a-45fa-ad8e-32f85aae1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x1\": np.random.randint(0, 10, 100).astype(float), \n",
    "                   \"x2\": np.random.randint(0, 3, 100).astype(float)})\n",
    "df[\"y\"] = df[\"x1\"] + df[\"x2\"] + np.random.rand(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf570f0-7057-49de-8c29-8862d06a4035",
   "metadata": {},
   "source": [
    "Let's convert pandas dataframe to Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5fc3a-5866-42c9-aaea-50e937a3fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c0e3e-acd1-45bf-94fd-094cfd786b63",
   "metadata": {},
   "source": [
    "Recall that seed in Spark is not truly deterministic overall (because everytime we might have new partitions), just deterministic at the partition level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842274a-33ae-4d44-9b2d-a72b7661caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.75, 0.25], seed=42)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8e0c4-6699-42d7-84af-935f1ee5652d",
   "metadata": {},
   "source": [
    "Let's write data to Parquet format and read the data from the Parquet file.\n",
    "We need to now use `mode(\"ignore\")` to make sure that we work with the deterministic sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab1841-8026-43be-8e0d-fdb29b086d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/train.parquet\")\n",
    "test.write.format(\"parquet\").mode(\"ignore\").save(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142a7fb-c4af-46d1-a884-d3a43d54543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/train.parquet\")\n",
    "test = spark.read.format(\"parquet\").load(\"hdfs://nn:9000/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd68d0-b44f-4d90-85f0-f7841a128a36",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "- `DecisionTreeRegressor`: unfit model\n",
    "- `DecisionTreeRegressionModel`: fitted model\n",
    "    - In Spark, names ending in \"Model\" are the fitted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947e68f-492a-4e2d-960f-3bb9cfe66bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"y\")\n",
    "\n",
    "model = dt.fit(va.transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a4fba-6010-473f-ac35-e89acbeeb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dt), type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b769e-b922-463b-85e6-812556a7153d",
   "metadata": {},
   "source": [
    "### Lecture starts here\n",
    "\n",
    "### Pipelines\n",
    "\n",
    "- `Pipeline`: unfit model\n",
    "- `PipelineModel`: fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f66d2a-105c-4527-8408-cc77901087e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c262e57-ebf8-462b-9d02-79ad44067905",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(stages=[va, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7209616-1115-4de7-87fc-8dae54c9a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipe.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f0f96-3910-4634-b84e-5963bf8a3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipe), type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b6270-8831-484a-bd85-9d87a1dad75a",
   "metadata": {},
   "source": [
    "### Pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd21d7-a9bd-41f2-96e9-453298a42c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700902d2-ea66-4373-83ed-664b616a41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.stages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df04d8f-57fd-4165-a1e9-cb5bd0c9dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.stages[1].toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04edc0de-efcc-4323-a59e-d0485fd038be",
   "metadata": {},
   "source": [
    "### Saving pipeline to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00b372-0b11-4252-93d4-a9a58290e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"hdfs://nn:9000/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e450a3e-2e39-4bfd-95c3-632150d35706",
   "metadata": {},
   "source": [
    "Let's try `ls` on HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d723c-6e5e-4d6f-b566-afe4613b0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs://nn:9000/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2eb87-967e-4c3c-bb4d-f4edb89a3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs://nn:9000/model/stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8c4db-5133-493a-970d-a29b4c333af4",
   "metadata": {},
   "source": [
    "Let's load the model from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192bb7b-79db-4c47-a461-d797e0111a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load(\"hdfs://nn:9000/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02009257-3807-41ad-98a8-62c1e3fef71b",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf71ad0-c9e0-4f20-9d73-ea49dd8a5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922fd3c-a5a5-4237-897d-9b8bde4a659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ad942-2cbc-42fa-b011-78038977e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30a381-6487-43ce-a66e-8fd3b7685509",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed67af-8547-4391-97a8-e5a8287a032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db39064-93c8-4d52-8ed7-8a1bc862c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2score = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"y\", metricName=\"r2\")\n",
    "r2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2595cb5-4812-438b-8213-8b94d5efb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2score.evaluate(model.transform(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
